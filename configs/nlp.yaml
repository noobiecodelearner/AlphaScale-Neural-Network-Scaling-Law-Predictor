domain: nlp
dataset: dbpedia
data_path: data/raw/dbpedia
num_classes: 14
vocab_size: 30522
max_seq_len: 128

dataset_fractions:
  - 0.25
  - 0.5
  - 1.0

model_scales:
  layer_dim_pairs:
    - [1, 16]    # ~130K params
    - [1, 32]    # ~520K params  
    - [1, 48]    # ~1.1M params
    - [1, 64]    # ~2M params
    - [2, 128]   # ~4M params
    - [4, 256]   # ~11M params

training:
  epochs: 15
  batch_size: 128
  optimizer: adamw
  learning_rate: 0.0001
  weight_decay: 0.00001

energy:
  gpu_wattage: 250